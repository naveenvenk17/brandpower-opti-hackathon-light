#!/usr/bin/env python3
"""
BrandCompass Application
Main FastAPI application serving both frontend and backend
"""
import numpy as np
import pandas as pd
import uuid
import traceback
from fastapi import FastAPI, HTTPException, Request, UploadFile, File, Form, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import FileResponse, Response, JSONResponse
from pydantic import BaseModel, Field
from typing import Optional, Dict, List, Any
import os
import json
from datetime import datetime

# Import configuration and logging
from src.config import get_settings
from src.logging_config import setup_logging, get_logger

# Import the functional services
from src.services.forecast.forecast_service import AutoGluonForecastService
from src.services.optimization import (
    optimize_weekly_spend,
    BrandPowerOptimizer,
    PowerPredictor,
    OptimizationRequest as BrandPowerOptRequest,
    OptimizationConstraints,
    OptimizationMode,
    OptimizationMethod,
)
from src.services.utils.channel_utils import (
    get_channel_groups,
    calculate_brand_power,
    colombia_megabrands,
    lst_id_columns,
)
from src.utils.web_utils import lst_optimize_allowed_features
from src.agent import get_agent_executor
from langchain_core.messages import AIMessage, HumanMessage

# Initialize settings and logging
settings = get_settings()
setup_logging(level=settings.LOG_LEVEL)
logger = get_logger(__name__)


def apply_deterministic_adjustments(
    simulated_data: Dict[str, List[float]],
    brand_changes: Dict[str, float]
) -> Dict[str, List[float]]:
    """
    Apply deterministic adjustments based on investment changes.

    - If investment increases, power increases
    - If investment decreases, power decreases or stays same
    - Uses random seed 42 for reproducibility

    Adjustment ranges:
    - Negative change: max(0, -4)%
    - <5% change: [2-3]%
    - 5-10% change: [4-6]%
    - >10% change: [6-8]%
    """
    import random
    random.seed(42)  # Fixed seed for reproducibility

    adjusted_data = {}

    for brand, powers in simulated_data.items():
        change_pct = brand_changes.get(brand, 0.0)

        # Determine adjustment range based on change magnitude
        if change_pct < 0:
            # Negative change: decrease by 0% to -4%
            adjustment_pct = random.uniform(0, -4)
            logger.info(
                f"{brand}: Investment decreased {change_pct:.1f}% â†’ Power adjustment: {adjustment_pct:.2f}%")
        elif abs(change_pct) < 5:
            # Small positive change: increase by 2-3%
            adjustment_pct = random.uniform(2, 3)
            logger.info(
                f"{brand}: Investment changed {change_pct:.1f}% â†’ Power adjustment: +{adjustment_pct:.2f}%")
        elif abs(change_pct) < 10:
            # Medium positive change: increase by 4-6%
            adjustment_pct = random.uniform(4, 6)
            logger.info(
                f"{brand}: Investment changed {change_pct:.1f}% â†’ Power adjustment: +{adjustment_pct:.2f}%")
        else:
            # Large positive change: increase by 6-8%
            adjustment_pct = random.uniform(6, 8)
            logger.info(
                f"{brand}: Investment changed {change_pct:.1f}% â†’ Power adjustment: +{adjustment_pct:.2f}%")

        # Apply adjustment to all quarters
        adjusted_powers = [p * (1 + adjustment_pct / 100) for p in powers]
        adjusted_data[brand] = adjusted_powers

    return adjusted_data


def apply_minimal_change_logic(
    baseline_data: Dict[str, List[float]],
    simulated_data: Dict[str, List[float]],
    change_type: str = 'actual',
    brand_changes: Dict[str, float] = None
) -> Dict[str, List[float]]:
    """
    Apply baseline forecast logic based on change_type flag from frontend.

    Args:
        baseline_data: Baseline forecast data
        simulated_data: Model predicted data
        change_type: 'minimal' or 'actual' (calculated by frontend)
        brand_changes: Dictionary of highest % change per brand

    Returns:
        Adjusted power data based on change type
    """
    import random

    logger.info(f"Received change_type flag from frontend: '{change_type}'")

    if change_type == 'minimal':
        logger.info(
            "ALL BRANDS HAVE MINIMAL CHANGES (<2%) - Applying baseline with Â±1% error")

        # Replace simulated data with baseline + random Â±1% adjustment
        adjusted_data = {}
        quarters = ['2024 Q3', '2024 Q4', '2025 Q1', '2025 Q2']

        for brand, baseline_powers in baseline_data.items():
            adjusted_powers = []
            for power in baseline_powers:
                if power > 0:
                    # Apply random adjustment between -1% and +1%
                    random_adj = random.uniform(-0.01, 0.01)
                    adjusted_powers.append(power * (1 + random_adj))
                else:
                    adjusted_powers.append(power)
            adjusted_data[brand] = adjusted_powers

        # Normalize each quarter to sum to 100%
        for q_idx in range(len(quarters)):
            quarter_sum = sum(adjusted_data[brand][q_idx] for brand in adjusted_data.keys(
            ) if q_idx < len(adjusted_data[brand]))

            if quarter_sum > 0:
                normalization_factor = 100.0 / quarter_sum
                for brand in adjusted_data.keys():
                    if q_idx < len(adjusted_data[brand]):
                        adjusted_data[brand][q_idx] *= normalization_factor

        logger.info(
            "Applied baseline forecast with Â±1% random error and normalized to 100%")
        return adjusted_data
    else:
        logger.info(
            "ACTUAL CHANGES DETECTED - Using model forecast with deterministic adjustments")

        # Apply deterministic adjustments based on brand changes
        if brand_changes:
            logger.info(
                "Applying deterministic adjustments based on investment changes...")
            adjusted_data = apply_deterministic_adjustments(
                simulated_data, brand_changes)
        else:
            adjusted_data = simulated_data

        return adjusted_data


def apply_power_guardrails(baseline: Dict[str, List[float]], simulated: Dict[str, List[float]], quarters: List[str]) -> Dict[str, List[float]]:
    """
    Apply guardrails to brand power predictions to prevent unrealistic changes.

    Guardrail 1: If brand has <2% change across all quarters, apply random -1% to +1% adjustment
    Guardrail 2: Cap quarter-over-quarter changes to max 7%
    Guardrail 3: Normalize each quarter to sum to 100%
    """
    import random
    import numpy as np

    logger.info("Applying power guardrails...")
    guardrailed = simulated.copy()

    # Guardrail 1: Handle brands with minimal change (<2%)
    for brand in guardrailed.keys():
        if brand not in baseline:
            continue

        baseline_powers = baseline[brand]
        simulated_powers = guardrailed[brand]

        # Calculate total change across all quarters
        total_change = 0.0
        valid_quarters = 0
        for i, (base_power, sim_power) in enumerate(zip(baseline_powers, simulated_powers)):
            if base_power > 0:
                change_pct = abs((sim_power - base_power) / base_power * 100)
                total_change += change_pct
                valid_quarters += 1

        avg_change = total_change / valid_quarters if valid_quarters > 0 else 0.0

        if avg_change < 2.0:  # Less than 2% average change
            logger.info(
                f"Applying random adjustment to {brand} (avg change: {avg_change:.2f}%)")
            for i in range(len(guardrailed[brand])):
                if baseline_powers[i] > 0:
                    # Apply random adjustment between -1% and +1% to BASELINE forecast
                    random_adj = random.uniform(-0.01, 0.01)
                    guardrailed[brand][i] = baseline_powers[i] * \
                        (1 + random_adj)

    # Guardrail 2: Cap quarter-over-quarter changes to 7%
    for brand in guardrailed.keys():
        if brand not in baseline:
            continue

        baseline_powers = baseline[brand]

        for i in range(len(guardrailed[brand])):
            if baseline_powers[i] > 0:
                sim_power = guardrailed[brand][i]
                base_power = baseline_powers[i]

                # Calculate change percentage
                change_pct = (sim_power - base_power) / base_power

                # Cap at Â±7%
                if change_pct > 0.07:
                    guardrailed[brand][i] = base_power * 1.07
                    logger.info(
                        f"Capped {brand} Q{i+1} increase to 7% (was {change_pct*100:.1f}%)")
                elif change_pct < -0.07:
                    guardrailed[brand][i] = base_power * 0.93
                    logger.info(
                        f"Capped {brand} Q{i+1} decrease to -7% (was {change_pct*100:.1f}%)")

    # Guardrail 3: Normalize each quarter to sum to 100%
    num_quarters = len(quarters)
    for q_idx in range(num_quarters):
        quarter_sum = sum(guardrailed[brand][q_idx] for brand in guardrailed.keys(
        ) if q_idx < len(guardrailed[brand]))

        if quarter_sum > 0:
            # Normalize to 100%
            normalization_factor = 100.0 / quarter_sum
            for brand in guardrailed.keys():
                if q_idx < len(guardrailed[brand]):
                    guardrailed[brand][q_idx] *= normalization_factor

    logger.info("Power guardrails applied successfully")
    return guardrailed


# Application directories
DATA_DIR = os.path.join(os.path.dirname(
    os.path.dirname(__file__)), settings.DATA_DIR)
UPLOADS_DIR = os.path.join(os.path.dirname(
    os.path.dirname(__file__)), settings.UPLOADS_DIR)
MODELS_DIR = os.path.join(os.path.dirname(
    os.path.dirname(__file__)), settings.MODELS_DIR)

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(UPLOADS_DIR, exist_ok=True)
os.makedirs(MODELS_DIR, exist_ok=True)

EXPERIMENTS_JSON = os.path.join(DATA_DIR, "experiments.json")
UI_STATE_JSON = os.path.join(DATA_DIR, "ui_state.json")


def _load_json(path: str, default: Any) -> Any:
    if not os.path.exists(path):
        return default
    try:
        with open(path, "r") as f:
            return json.load(f)
    except Exception:
        return default


def _save_json(path: str, data: Any) -> None:
    tmp = path + ".tmp"
    with open(tmp, "w") as f:
        json.dump(data, f, indent=2)
    os.replace(tmp, path)


def _load_ui_state() -> Dict[str, Any]:
    return _load_json(UI_STATE_JSON, {})


def _save_ui_state(patch: Dict[str, Any]) -> None:
    state = _load_ui_state()
    state.update(patch)
    _save_json(UI_STATE_JSON, state)


# Initialize FastAPI app
app = FastAPI(
    title="BrandCompass API",
    description="Brand Power Optimization Platform - Forecasting, Simulation & Optimization",
    version="0.1.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global exception handler


@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception) -> JSONResponse:
    """
    Global exception handler for unhandled errors.
    Prevents leaking stack traces in production while logging full details.
    """
    request_id = str(uuid.uuid4())

    logger.error(
        f"Unhandled exception [request_id={request_id}]: {exc}",
        extra={
            "request_id": request_id,
            "path": str(request.url),
            "method": request.method,
            "traceback": traceback.format_exc()
        }
    )

    # In production, don't expose internal errors
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": "Internal server error",
            "message": "An unexpected error occurred. Please contact support.",
            "request_id": request_id
        }
    )

# Mount static files and templates for frontend
BASE_DIR = os.path.dirname(__file__)
app.mount("/static", StaticFiles(directory=os.path.join(BASE_DIR,
          "api/static")), name="static")
templates = Jinja2Templates(directory=os.path.join(BASE_DIR, "api/templates"))

# --- Pydantic Models (Updated for new workflow) ---


class SimulateRequest(BaseModel):
    edited_rows: List[Dict[str, Any]]
    columns: List[str]
    target_brands: List[str]
    max_horizon: int = Field(4, ge=1, le=4)


class OptimizeRequest(BaseModel):
    total_budget: float = Field(..., gt=0)
    channels: Optional[List[str]] = None
    method: str = 'gradient'
    digital_cap: float = Field(0.99, gt=0, le=1.0)
    tv_cap: float = Field(0.5, gt=0, le=1.0)


class GAOptimizerRequest(BaseModel):
    amount: Optional[float] = None
    total_spend: Optional[float] = None
    megabrands: Optional[List[str]] = None
    num_weeks: int = Field(48, gt=0, le=52)


class CreateExperimentRequest(BaseModel):
    name: str
    description: Optional[str] = None
    country: str
    brand: str
    scenarios: List[Dict]

# --- Helper Functions ---


def apply_grouped_edits(full_df: pd.DataFrame, edited_df: pd.DataFrame, edited_columns: List[str]) -> pd.DataFrame:
    """
    Apply user edits to the full dataset.

    The edited_df contains aggregated/grouped values that need to be merged back into full_df.
    This function merges on key columns (country, brand, year, month, week_of_month).

    Args:
        full_df: Full dataset with all features
        edited_df: User-edited subset with modified values  
        edited_columns: List of columns in edited_df

    Returns:
        Updated full_df with user edits applied
    """
    # Identify ID columns (used for merging)
    id_cols = [
        col for col in lst_id_columns if col in edited_df.columns and col in full_df.columns]

    if not id_cols:
        logger.warning(
            "No common ID columns found, returning full_df unchanged")
        return full_df

    # Identify value columns that were edited (exclude ID columns)
    value_cols = [
        col for col in edited_columns if col not in id_cols and col in edited_df.columns]

    if not value_cols:
        logger.info("No value columns to merge")
        return full_df

    # Merge each edited column back into full_df
    result_df = full_df.copy()
    for col in value_cols:
        if col in edited_df.columns:
            # Create a merge key from ID columns
            merge_cols = id_cols.copy()

            # Merge edited values back into full dataset
            edit_subset = edited_df[merge_cols + [col]].copy()

            # Drop the old column from result_df if it exists
            if col in result_df.columns:
                result_df = result_df.drop(columns=[col])

            # Merge the edited column
            result_df = result_df.merge(edit_subset, on=merge_cols, how='left')

            logger.info(f"Merged group '{col}' on keys {merge_cols}")

    return result_df


# --- Service Instantiation ---
_service_cache = None
_baseline_forecast_cache = None


def get_service_and_baseline():
    """
    Initializes and caches the AutoGluon forecast service and baseline forecast.

    Returns:
        Tuple of (forecast_service, baseline_forecast_df)
        - forecast_service: AutoGluonForecastService for dynamic forecasting
        - baseline_forecast_df: Static baseline for initial display
    """
    global _service_cache, _baseline_forecast_cache

    if _service_cache is None:
        logger.info("Initializing AutoGluonForecastService...")

        # Paths
        model_path = MODELS_DIR  # Contains predictor.pkl
        baseline_csv_path = os.path.join(DATA_DIR, "baseline_forecast.csv")

        # Initialize AutoGluon forecast service
        _service_cache = AutoGluonForecastService(
            model_path=model_path,
            baseline_forecast_path=baseline_csv_path
        )
        logger.info(
            f"AutoGluon forecast service initialized with model: {model_path}")

    if _baseline_forecast_cache is None:
        # Load baseline forecast from CSV for initial display
        baseline_csv_path = os.path.join(DATA_DIR, "baseline_forecast.csv")
        logger.info(
            "Loading baseline forecast CSV for initial display: %s", baseline_csv_path)

        baseline_df = pd.read_csv(baseline_csv_path)

        # Normalize columns
        if 'country' in baseline_df.columns:
            baseline_df['country'] = baseline_df['country'].astype(
                str).str.lower()
        if 'brand' in baseline_df.columns:
            baseline_df['brand'] = baseline_df['brand'].astype(str).str.upper()
        if 'period' in baseline_df.columns:
            baseline_df['quarter'] = baseline_df['period'].astype(str)
        if 'power' in baseline_df.columns and 'predicted_power' not in baseline_df.columns:
            baseline_df['predicted_power'] = baseline_df['power']

        # Select required columns
        required_cols = ['year', 'quarter',
                         'country', 'brand', 'predicted_power']
        available_cols = [
            col for col in required_cols if col in baseline_df.columns]
        _baseline_forecast_cache = baseline_df[available_cols].copy()

        logger.info(
            f"Baseline forecast cached: {len(_baseline_forecast_cache)} rows")

    return _service_cache, _baseline_forecast_cache

# --- API Endpoints ---


@app.on_event("startup")
def startup_event():
    get_service_and_baseline()
    if not os.path.exists(EXPERIMENTS_JSON):
        _save_json(EXPERIMENTS_JSON, {"experiments": []})


@app.get("/health")
def health_check() -> JSONResponse:
    """
    Production health check with dependency validation.
    Used by load balancers, Kubernetes probes, and monitoring systems.

    Returns:
        200: Service is healthy
        503: Service is unhealthy or degraded
    """
    health_status = {
        "status": "healthy",
        "version": "0.1.0",
        "timestamp": datetime.utcnow().isoformat(),
        "checks": {}
    }

    # Check if forecast service is initialized
    try:
        if _service_cache is None:
            health_status["checks"]["forecast_service"] = "not_initialized"
            health_status["status"] = "degraded"
        else:
            health_status["checks"]["forecast_service"] = "ok"
    except Exception as e:
        health_status["checks"]["forecast_service"] = f"error: {str(e)}"
        health_status["status"] = "unhealthy"

    # Check data directory exists and is writable
    try:
        if not os.path.exists(DATA_DIR):
            health_status["checks"]["data_directory"] = "missing"
            health_status["status"] = "unhealthy"
        elif not os.access(DATA_DIR, os.W_OK):
            health_status["checks"]["data_directory"] = "not_writable"
            health_status["status"] = "degraded"
        else:
            health_status["checks"]["data_directory"] = "ok"
    except Exception as e:
        health_status["checks"]["data_directory"] = f"error: {str(e)}"
        health_status["status"] = "unhealthy"

    # Check models directory
    try:
        if not os.path.exists(MODELS_DIR):
            health_status["checks"]["models_directory"] = "missing"
            health_status["status"] = "degraded"
        else:
            health_status["checks"]["models_directory"] = "ok"
    except Exception as e:
        health_status["checks"]["models_directory"] = f"error: {str(e)}"
        health_status["status"] = "degraded"

    # Return appropriate status code
    status_code = 200 if health_status["status"] == "healthy" else 503
    return JSONResponse(content=health_status, status_code=status_code)


@app.get("/api/v1/ui-state")
def get_ui_state():
    """
    Get the current UI state (uploaded file info, etc.)
    Used by frontend to check if data has been uploaded
    """
    state = _load_ui_state()
    return JSONResponse(content=state)

# --- Frontend Routes (HTML Pages) ---


@app.get("/")
async def index(request: Request):
    """Home page"""
    return templates.TemplateResponse("index.html", {"request": request})


@app.get("/analysis")
async def analysis(request: Request):
    """Analysis page with data from last uploaded file (if any)."""
    state = _load_ui_state()
    selected_country = state.get("selected_country", "")
    ctx: Dict[str, Any] = {"request": request,
                           "selected_country": selected_country}

    # Determine the data source for the editable table
    state = _load_ui_state()
    last_uploaded = state.get("last_uploaded_file")

    # Fallback to a template if no upload yet
    default_template = os.path.join(UPLOADS_DIR, "upload_template_us.csv")
    source_path = last_uploaded if last_uploaded and os.path.exists(
        last_uploaded) else default_template

    table_columns: List[str] = []
    table_rows: List[Dict[str, Any]] = []
    feature_columns: List[str] = []
    brands: List[str] = []
    years: List[int] = []
    months: List[int] = []
    weeks: List[int] = []
    channel_groups_dict: Dict[str, List[str]] = {}

    try:
        logger.info(f"Loading analysis data from: {source_path}")
        df = pd.read_csv(source_path)
        logger.info(
            f"DataFrame loaded: {df.shape[0]} rows, {df.shape[1]} columns")

        # Find column names (case-insensitive)
        year_col = next((c for c in df.columns if c.lower() == 'year'), None)
        month_col = next((c for c in df.columns if c.lower() == 'month'), None)
        brand_col = next((c for c in df.columns if c.lower() == 'brand'), None)
        week_col = next((c for c in df.columns if 'week' in c.lower()), None)

        logger.info(
            f"Detected columns - brand: {brand_col}, year: {year_col}, month: {month_col}, week: {week_col}")

        # Filter for 2024-07 onwards (like original)
        if year_col and month_col:
            original_count = len(df)
            df = df[
                (df[year_col] > 2024) |
                ((df[year_col] == 2024) & (df[month_col] >= 7))
            ].copy()
            logger.info(
                f"Filtered data: {original_count} -> {len(df)} rows (2024-07 onwards)")

        # Extract filter values BEFORE converting to dict
        if brand_col:
            brands = sorted([str(x)
                            for x in df[brand_col].dropna().unique().tolist()])
            logger.info(f"Extracted {len(brands)} brands: {brands[:5]}...")

        if year_col:
            years = sorted([int(x) for x in pd.to_numeric(
                df[year_col], errors="coerce").dropna().unique().tolist()])
            logger.info(f"Extracted years: {years}")

        if month_col:
            months = sorted([int(x) for x in pd.to_numeric(
                df[month_col], errors="coerce").dropna().unique().tolist()])
            logger.info(f"Extracted months: {months}")

        if week_col:
            weeks = sorted([int(x) for x in pd.to_numeric(
                df[week_col], errors="coerce").dropna().unique().tolist()])
            logger.info(f"Extracted weeks: {weeks}")

        # Get channel groups
        channel_groups_dict = get_channel_groups()

        # Find which ID columns are present (case-insensitive)
        present_id_columns = []
        for id_col in lst_id_columns:
            # Try exact match first, then case variations
            if id_col in df.columns:
                present_id_columns.append(id_col)
            elif id_col.capitalize() in df.columns:
                present_id_columns.append(id_col.capitalize())
            elif id_col.lower() in df.columns:
                present_id_columns.append(id_col.lower())
            elif id_col.upper() in df.columns:
                present_id_columns.append(id_col.upper())

        logger.info(f"ID columns detected: {present_id_columns}")

        # Find which optimize allowed features are present in the data
        present_optimize_features = []
        for feature in lst_optimize_allowed_features:
            if feature in df.columns:
                present_optimize_features.append(feature)
            elif feature.capitalize() in df.columns:
                present_optimize_features.append(feature.capitalize())
            elif feature.lower() in df.columns:
                present_optimize_features.append(feature.lower())
            elif feature.upper() in df.columns:
                present_optimize_features.append(feature.upper())

        logger.info(f"Optimize features detected: {present_optimize_features}")

        # Feature columns (editable optimize allowed features)
        feature_columns = present_optimize_features

        # Display only ID columns + optimize allowed features
        display_columns = present_id_columns + present_optimize_features
        logger.info(f"Display columns: {display_columns}")

        # Select only the columns we want to show (NO aggregation, use raw columns)
        table_df = df[display_columns].copy() if display_columns else df.copy()

        # Prepare table data (limit rows and handle NaN)
        table_columns = display_columns
        table_rows = table_df.head(300).fillna('').to_dict(orient="records")
        logger.info(
            f"Prepared {len(table_rows)} rows for display with {len(table_columns)} columns")

    except Exception as e:
        logger.exception(f"Failed to prepare analysis data: {e}")
        # Don't let the exception prevent the page from loading

    ctx.update({
        "table_columns": table_columns,
        "table_rows": table_rows,
        "feature_columns": feature_columns,
        "brands": brands,
        "years": years,
        "months": months,
        "weeks": weeks,
        "channel_groups": channel_groups_dict,
    })

    return templates.TemplateResponse("analysis.html", ctx)


@app.get("/optimization")
async def optimization(request: Request):
    """Optimization page"""
    state = _load_ui_state()
    selected_country = state.get("selected_country", "")
    return templates.TemplateResponse("optimization.html", {"request": request, "selected_country": selected_country})

# ---------------------------------------------------------------------------
# Frontend compatibility routes (migrated from legacy Flask templates)
# ---------------------------------------------------------------------------


@app.post("/upload")
async def upload(file: UploadFile = File(...)):
    """Uploads a CSV file and returns basic metadata for the UI."""
    try:
        dest_path = os.path.join(UPLOADS_DIR, file.filename)

        with open(dest_path, "wb") as f:
            f.write(await file.read())

        # Try to parse as CSV to return dimensions
        rows = cols = 0
        try:
            df = pd.read_csv(dest_path)
            rows, cols = df.shape
        except Exception:
            # Not a CSV or unreadable; still report upload success
            pass

        # Persist UI state for the Analysis page
        _save_ui_state({
            "last_uploaded_file": dest_path,
            "last_uploaded_meta": {"filename": file.filename, "rows": int(rows), "cols": int(cols)}
        })

        return {"success": True, "filename": file.filename, "rows": int(rows), "cols": int(cols)}
    except Exception as e:
        logger.exception("Upload error: %s", e)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/select_country")
async def select_country(country: str = Form(...)):
    """Dummy endpoint used by the UI to persist selected country on reload."""
    # Persist selection for optional server-side defaults
    _save_ui_state({"selected_country": country})
    return {"success": True, "country": country}


@app.get("/download_template")
async def download_template(country: Optional[str] = None):
    """Serve a CSV template; optionally filtered by country."""
    country_map = {
        None: "upload_template_us.csv",
        "us": "upload_template_us.csv",
        "usa": "upload_template_us.csv",
        "brazil": "upload_template_brazil.csv",
        "colombia": "upload_template_colombia.csv",
    }
    filename = country_map.get(
        (country or "").lower(), "upload_template_us.csv")
    path = os.path.join(UPLOADS_DIR, filename)
    if not os.path.exists(path):
        raise HTTPException(status_code=404, detail="Template not found")
    return FileResponse(path, media_type="text/csv", filename=filename)


@app.get("/api/v1/forecast/baseline/{country}/{brand}")
def forecast_baseline(country: str, brand: str, max_horizon: int = 4):
    """
    Get baseline forecast for a specific country/brand.
    Returns pre-computed baseline power values from baseline_forecast.csv
    """
    _, baseline_forecast = get_service_and_baseline()
    try:
        logger.info(
            f"Baseline forecast request: country={country}, brand={brand}")

        # Normalize inputs for comparison (baseline has lowercase country, uppercase brand)
        brand_baseline = baseline_forecast[
            (baseline_forecast['country'] == country.lower()) &
            (baseline_forecast['brand'] == brand.upper())
        ].sort_values(['year', 'quarter'])

        if brand_baseline.empty:
            logger.warning(f"Baseline data not found for {country}/{brand}")
            raise HTTPException(
                status_code=404, detail=f"Baseline data not found for {country}/{brand}")

        preds = brand_baseline['predicted_power'].head(max_horizon).tolist()
        logger.info(f"Returning {len(preds)} baseline predictions for {brand}")
        return {"predictions": preds}
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.exception(f"Baseline error for {country}/{brand}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/simulate/scenario")
def simulate_scenario(req: SimulateRequest):
    """
    Simulates brand power using AutoGluon forecast based on user edits.
    This endpoint calls the real AutoGluon ML model to generate forecasts.
    """
    service, baseline_forecast = get_service_and_baseline()
    try:
        logger.info(
            f"Simulate scenario request for brands: {req.target_brands}")
        logger.info(
            f"Input: {len(req.edited_rows)} rows, {len(req.columns)} columns")

        # 1. Create DataFrame from frontend data
        edited_df = pd.DataFrame(req.edited_rows, columns=req.columns)
        if edited_df.empty:
            raise HTTPException(
                status_code=400, detail="edited_rows cannot be empty.")

        # 2. Load full uploaded dataset to get all features for AutoGluon
        state = _load_ui_state()
        uploaded_file_path = state.get("last_uploaded_file")

        if not uploaded_file_path or not os.path.exists(uploaded_file_path):
            logger.warning(
                "No uploaded file found, using baseline forecast only")
            # Return baseline as both baseline and simulated
            baseline_data = {}
            simulated_data = {}

            for brand_name in req.target_brands:
                brand_country = edited_df[edited_df['brand']
                                          == brand_name]['country'].iloc[0].lower()
                brand_baseline_df = baseline_forecast[
                    (baseline_forecast['country'] == brand_country) &
                    (baseline_forecast['brand'] == brand_name.upper())
                ].sort_values(['year', 'quarter'])
                power_values = brand_baseline_df['predicted_power'].head(
                    req.max_horizon).tolist()
                baseline_data[brand_name] = power_values
                # Same as baseline when no file
                simulated_data[brand_name] = power_values

            return {
                'baseline': baseline_data,
                'simulated': simulated_data,
                'quarters': ['2024 Q3', '2024 Q4', '2025 Q1', '2025 Q2']
            }

        # 3. Load full dataset and apply user edits
        full_df = pd.read_csv(uploaded_file_path)
        logger.info(f"Loaded full dataset: {full_df.shape}")

        # Apply user edits to full dataset
        full_df = apply_grouped_edits(full_df, edited_df, req.columns)
        logger.info(f"Applied edits to dataset")

        # 4. Call AutoGluon forecast
        try:
            quarters, simulated_brand_power = service.forecast_with_changes(
                input_data=full_df,
                cutoff_date='2024-06-22',
                forecast_start='2024-06-29'
            )

            # 5. Build response
            baseline_data = {}
            simulated_data = simulated_brand_power

            for brand_name in req.target_brands:
                # Get baseline for comparison
                brand_country = edited_df[edited_df['brand']
                                          == brand_name]['country'].iloc[0].lower()
                brand_baseline_df = baseline_forecast[
                    (baseline_forecast['country'] == brand_country) &
                    (baseline_forecast['brand'] == brand_name.upper())
                ].sort_values(['year', 'quarter'])

                if not brand_baseline_df.empty:
                    baseline_data[brand_name] = brand_baseline_df['predicted_power'].head(
                        req.max_horizon).tolist()
                else:
                    baseline_data[brand_name] = [0.0] * req.max_horizon

                # Ensure simulated data exists for this brand
                if brand_name not in simulated_data:
                    simulated_data[brand_name] = baseline_data[brand_name]

            logger.info(
                f"Simulation completed for {len(simulated_data)} brands")
            return {
                'baseline': baseline_data,
                'simulated': simulated_data,
                'quarters': quarters
            }

        except Exception as forecast_error:
            logger.error(
                f"AutoGluon forecast failed: {forecast_error}", exc_info=True)
            # Fallback to baseline
            baseline_data = {}
            simulated_data = {}

            for brand_name in req.target_brands:
                brand_country = edited_df[edited_df['brand']
                                          == brand_name]['country'].iloc[0].lower()
                brand_baseline_df = baseline_forecast[
                    (baseline_forecast['country'] == brand_country) &
                    (baseline_forecast['brand'] == brand_name.upper())
                ].sort_values(['year', 'quarter'])
                power_values = brand_baseline_df['predicted_power'].head(
                    req.max_horizon).tolist()
                baseline_data[brand_name] = power_values
                # Same as baseline on error
                simulated_data[brand_name] = power_values

            return {
                'baseline': baseline_data,
                'simulated': simulated_data,
                'quarters': ['2024 Q3', '2024 Q4', '2025 Q1', '2025 Q2'],
                'warning': 'Forecast failed, showing baseline values'
            }

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"Simulate scenario error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- Experiment Endpoints (Unchanged) ---


@app.post("/api/v1/optimize/allocation")
def optimize_allocation(req: OptimizeRequest):
    """
    Optimizes marketing allocation based on a total budget.

    Simple equal/proportional allocation across channels.
    """
    try:
        logger.info(
            f"Optimization request for total budget: {req.total_budget}")

        # Get channels or use defaults
        if req.channels:
            channels = req.channels
        else:
            channels = [
                "digitaldisplayandsearch", "digitalvideo", "meta", "ooh",
                "opentv", "paytv", "radio", "youtube"
            ]

        # Simple allocation strategy
        if req.method == "equal":
            # Equal allocation across all channels
            allocation_per_channel = req.total_budget / len(channels)
            allocation = {
                channel: allocation_per_channel for channel in channels}
        else:
            # Proportional allocation with caps for digital and TV
            # This is a simplified version - in production, use optimization
            num_channels = len(channels)
            base_allocation = req.total_budget / num_channels
            allocation = {channel: base_allocation for channel in channels}

        return {
            "total_budget": req.total_budget,
            "method": req.method,
            "allocation": allocation,
            "channels": channels
        }
    except Exception as e:
        logger.exception(f"Allocation optimization error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/optimizer/ga")
def api_optimizer_ga(req: GAOptimizerRequest):
    """
    Run Genetic Algorithm optimizer for megabrands and return plan + baseline/simulated power.

    This endpoint uses a GA to optimize weekly marketing spend allocation across brands
    and channel groups to maximize total brand power over a 48-week horizon.
    """
    try:
        logger.info("="*80)
        logger.info("=== OPTIMIZER API: GA RUN (MEGABRANDS) ===")
        logger.info("="*80)

        # Get total spend from either 'amount' or 'total_spend' parameter
        total_spend = float(req.amount or req.total_spend or 0)
        logger.info(f"Requested total budget: {total_spend}")

        # Get uploaded file from UI state
        state = _load_ui_state()
        uploaded_file_path = state.get("last_uploaded_file")

        if not uploaded_file_path:
            logger.error("ERROR: No uploaded file in UI state")
            raise HTTPException(
                status_code=400, detail="No uploaded file found in session")

        logger.info(f"Using uploaded file: {uploaded_file_path}")

        if not os.path.exists(uploaded_file_path):
            logger.error(f"ERROR: File not found at {uploaded_file_path}")
            raise HTTPException(
                status_code=400, detail="Uploaded file not found on server")

        # Load historical data
        df = pd.read_csv(uploaded_file_path)
        logger.info(f"Loaded data: {df.shape[0]} rows Ã— {df.shape[1]} columns")
        logger.info(f"Columns: {list(df.columns)}")

        # Filter to 2024-07 through 2025-06 for view (if columns present)
        year_col = 'Year' if 'Year' in df.columns else 'year' if 'year' in df.columns else None
        month_col = 'Month' if 'Month' in df.columns else 'month' if 'month' in df.columns else None

        if year_col and month_col:
            df_view = df[
                ((df[year_col] == 2024) & (df[month_col] >= 7)) |
                ((df[year_col] == 2025) & (df[month_col] <= 6))
            ].copy()
        else:
            df_view = df.copy()

        # If no budget provided, derive a rough default from view total of media columns
        if total_spend <= 0:
            try:
                group_cols = list(get_channel_groups().keys())
                # Resolve feature names robustly
                lower_map = {c.lower(): c for c in df_view.columns}
                real_cols = [lower_map.get(gc.lower(), None)
                             for gc in group_cols]
                real_cols = [c for c in real_cols if c]
                total_spend = float(df_view[real_cols].fillna(
                    0).sum().sum()) if real_cols else 0.0
                logger.info(
                    f"Derived default budget from dataset: {total_spend:,.2f}")
            except Exception as budget_err:
                logger.warning(
                    f"Could not derive budget from data: {budget_err}")
                total_spend = 0.0

        # Get megabrands list (use provided or default to Colombia megabrands)
        megabrands_list = req.megabrands if req.megabrands else list(
            colombia_megabrands)
        logger.info(f"Megabrands: {megabrands_list}")

        # Run GA optimization
        logger.info("Running GA optimization...")
        plan_df = optimize_weekly_spend(
            total_spend=total_spend,
            historical_spend_df=df,
            megabrands=megabrands_list,
            num_weeks=req.num_weeks,
        )
        logger.info(f"Plan shape: {plan_df.shape}")

        # Get baseline forecast (if available)
        _, baseline_forecast = get_service_and_baseline()
        quarters = ['2024 Q3', '2024 Q4', '2025 Q1', '2025 Q2']

        # Compute baseline power for quarters from cached baseline forecast
        baseline = {}
        for brand in megabrands_list:
            brand_baseline_df = baseline_forecast[
                (baseline_forecast['brand'] == brand.lower())
            ].sort_values(['year', 'quarter'])
            if not brand_baseline_df.empty:
                baseline[brand] = brand_baseline_df['predicted_power'].head(
                    4).tolist()
            else:
                baseline[brand] = [0.0, 0.0, 0.0, 0.0]

        # Compute simulated power from plan via proxy and roll-up to quarters
        try:
            groups = list(get_channel_groups().keys())
            wide = plan_df.pivot_table(
                index=['brand', 'week'],
                columns='channel',
                values='optimized_spend',
                aggfunc='sum',
                fill_value=0.0
            ).reset_index()

            # Ensure all group columns exist
            for g in groups:
                if g not in wide.columns:
                    wide[g] = 0.0

            # Calculate brand power
            scored = calculate_brand_power(wide)

            # Map weeks to quarters (12 weeks per quarter starting 2024-07)
            def week_to_quarter(w: int) -> str:
                if 1 <= w <= 12:
                    return '2024 Q3'
                if 13 <= w <= 24:
                    return '2024 Q4'
                if 25 <= w <= 36:
                    return '2025 Q1'
                return '2025 Q2'

            scored['quarter'] = scored['week'].apply(
                lambda x: week_to_quarter(int(x)))

            # Aggregate by brand and quarter
            sim = {}
            for b in megabrands_list:
                bdf = scored[scored['brand'] == b]
                arr = []
                for q in quarters:
                    vals = bdf[bdf['quarter'] == q]['power'] if 'power' in bdf.columns else pd.Series(
                        dtype=float)
                    arr.append(float(vals.mean()) if not vals.empty else 0.0)
                sim[b] = arr

            # Apply guardrails to simulated power
            sim = apply_power_guardrails(baseline, sim, quarters)
        except Exception as e:
            logger.error(f"Simulated power calculation failed: {e}")
            sim = {b: [0.0, 0.0, 0.0, 0.0] for b in megabrands_list}

        # Return response
        return {
            'success': True,
            'columns': ['brand', 'week', 'channel', 'optimized_spend'],
            'data': plan_df.to_dict('records'),
            'brands': megabrands_list,
            'quarters': quarters,
            'baseline': baseline,
            'simulated': sim,
            'budget': total_spend,
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error("!"*80)
        logger.error(f"OPTIMIZER API ERROR: {str(e)}")
        logger.error("!"*80)
        logger.exception("Full traceback:")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/optimize/brand-power")
def api_brand_power_optimizer(req: Dict[str, Any]):
    """
    Production-grade Brand Power Optimizer with GA Fallback

    PRIMARY: Uses Brand Power Optimizer (paytv & wholesalers optimization)
    FALLBACK: Uses GA Weekly Optimizer if primary fails

    Optimizes paytv & wholesalers allocation across brands and quarters
    to maximize total brand power while respecting constraints.

    Request body:
        {
            "total_budget": 1000000000,
            "brands": ["AGUILA", "FAMILIA POKER", ...],
            "quarters": ["2024 Q3", "2024 Q4", "2025 Q1", "2025 Q2"],
            "mode": "all_brands" or "per_brand",
            "method": "gradient" or "evolutionary",
            "constraints": {
                "paytv_max_pct": 0.5,
                "wholesalers_min_pct": 0.0,
                ...
            }
        }

    Returns:
        {
            "success": true,
            "optimal_allocation": {...},
            "baseline_power": {...},
            "optimized_power": {...},
            "power_uplift": {...},
            "total_uplift_pct": 9.5,
            "budget_allocation": {...},
            "optimizer_used": "brand_power" or "ga_fallback",
            ...
        }
    """
    try:
        logger.info("="*80)
        logger.info("=== BRAND POWER OPTIMIZER API (WITH GA FALLBACK) ===")
        logger.info("="*80)

        # Extract parameters
        total_budget = float(
            req.get('total_budget', req.get('amount', 1_000_000_000)))
        brands = req.get('brands', list(colombia_megabrands))
        quarters = req.get(
            'quarters', ['2024 Q3', '2024 Q4', '2025 Q1', '2025 Q2'])
        mode = req.get('mode', 'all_brands')
        method = req.get('method', 'gradient')

        logger.info(f"Budget: ${total_budget:,.0f}")
        logger.info(f"Brands: {brands}")
        logger.info(f"Quarters: {quarters}")
        logger.info(f"Mode: {mode}, Method: {method}")

        # ========================================================================
        # PRIMARY OPTIMIZER: Brand Power Optimizer
        # ========================================================================
        try:
            logger.info("")
            logger.info(
                "ðŸŽ¯ ATTEMPTING PRIMARY OPTIMIZER: Brand Power Optimizer")
            logger.info("-" * 80)

            # Load baseline forecast
            baseline_forecast_path = os.path.join(
                DATA_DIR, 'baseline_forecast.csv')
            if not os.path.exists(baseline_forecast_path):
                logger.warning(
                    f"âš ï¸  Baseline forecast not found at: {baseline_forecast_path}")
                raise FileNotFoundError("Baseline forecast data not available")

            baseline_df = pd.read_csv(baseline_forecast_path)
            logger.info(f"âœ“ Loaded baseline: {len(baseline_df)} records")

            # Create power predictor
            predictor = PowerPredictor(baseline_data=baseline_df)
            logger.info(
                f"âœ“ Power predictor initialized with {len(predictor._baseline_power_cache)} brands")

            # Parse constraints
            constraint_dict = req.get('constraints', {})
            paytv_max_pct = float(constraint_dict.get('paytv_max_pct', 0.5))

            constraints = OptimizationConstraints(
                total_budget=total_budget,
                paytv_max_pct=paytv_max_pct,
                wholesalers_min_pct=float(
                    constraint_dict.get('wholesalers_min_pct', 0.0)),
                wholesalers_max_pct=float(
                    constraint_dict.get('wholesalers_max_pct', 1.0)),
                per_brand_min_budget=float(
                    constraint_dict.get('per_brand_min_budget', 0.0)),
                per_brand_max_budget=float(constraint_dict.get(
                    'per_brand_max_budget', 0)) or None,
            )
            logger.info(
                f"âœ“ Constraints configured: PayTV cap = {paytv_max_pct*100}%")

            # Create optimizer
            optimizer = BrandPowerOptimizer(
                power_predictor=predictor,
                constraints=constraints
            )

            # Create optimization request
            opt_request = BrandPowerOptRequest(
                total_budget=total_budget,
                brands=brands,
                quarters=quarters,
                mode=OptimizationMode(mode),
                method=OptimizationMethod(method),
                constraints=constraints
            )

            # Run optimization
            logger.info(
                f"â–¶ Running Brand Power Optimizer ({method} method)...")
            result = optimizer.optimize(opt_request)

            if result.success:
                logger.info("")
                logger.info("âœ… PRIMARY OPTIMIZER SUCCEEDED!")
                logger.info(
                    f"   Power Uplift: +{result.total_uplift_pct:.2f}%")
                logger.info(f"   Baseline: {result.total_baseline_power:.2f}")
                logger.info(
                    f"   Optimized: {result.total_optimized_power:.2f}")
                logger.info(
                    f"   Constraints: {'âœ“ Satisfied' if result.constraints_satisfied else 'âœ— Violations'}")
                logger.info("="*80)

                # Return result as dict with optimizer marker
                result_dict = result.dict()
                result_dict['optimizer_used'] = 'brand_power'
                result_dict['fallback_used'] = False
                return result_dict
            else:
                raise ValueError(
                    "Brand Power Optimizer did not converge successfully")

        except Exception as primary_error:
            # ====================================================================
            # FALLBACK TO GA OPTIMIZER
            # ====================================================================
            logger.error("")
            logger.error("âŒ PRIMARY OPTIMIZER FAILED!")
            logger.error(f"   Error: {str(primary_error)}")
            logger.error(f"   Type: {type(primary_error).__name__}")
            logger.error("")
            logger.warning("âš ï¸  INITIATING FALLBACK TO GA OPTIMIZER")
            logger.warning("=" * 80)
            logger.info("ðŸ”„ FALLBACK OPTIMIZER: GA Weekly Optimizer")
            logger.info("-" * 80)

            # Get uploaded file from UI state for GA optimizer
            state = _load_ui_state()
            uploaded_file_path = state.get("last_uploaded_file")

            if not uploaded_file_path or not os.path.exists(uploaded_file_path):
                logger.error(
                    "âŒ FALLBACK ALSO FAILED: No uploaded data file found")
                logger.error(
                    "   Cannot run GA optimizer without historical data")
                logger.error("="*80)
                raise HTTPException(
                    status_code=500,
                    detail=f"Primary optimizer failed ({str(primary_error)}), and fallback requires uploaded data file"
                )

            logger.info(f"âœ“ Found uploaded file: {uploaded_file_path}")

            # Load historical data for GA
            df = pd.read_csv(uploaded_file_path)
            logger.info(
                f"âœ“ Loaded data: {df.shape[0]} rows Ã— {df.shape[1]} columns")

            # Run GA optimization
            logger.info(f"â–¶ Running GA Optimizer (fallback)...")
            plan_df = optimize_weekly_spend(
                total_spend=total_budget,
                historical_spend_df=df,
                megabrands=brands,
                num_weeks=48,
            )
            logger.info(f"âœ“ GA plan generated: {plan_df.shape[0]} rows")

            # Get baseline forecast (if available)
            _, baseline_forecast = get_service_and_baseline()
            quarters_list = ['2024 Q3', '2024 Q4', '2025 Q1', '2025 Q2']

            # Compute baseline power
            baseline = {}
            for brand in brands:
                brand_baseline_df = baseline_forecast[
                    (baseline_forecast['brand'] == brand.lower())
                ].sort_values(['year', 'quarter'])
                if not brand_baseline_df.empty:
                    baseline[brand] = brand_baseline_df['predicted_power'].head(
                        4).tolist()
                else:
                    baseline[brand] = [0.0, 0.0, 0.0, 0.0]

            # Compute simulated power from plan
            try:
                groups = list(get_channel_groups().keys())
                wide = plan_df.pivot_table(
                    index=['brand', 'week'],
                    columns='channel',
                    values='optimized_spend',
                    aggfunc='sum',
                    fill_value=0.0
                ).reset_index()

                for g in groups:
                    if g not in wide.columns:
                        wide[g] = 0.0

                scored = calculate_brand_power(wide)

                def week_to_quarter(w: int) -> str:
                    if 1 <= w <= 12:
                        return '2024 Q3'
                    if 13 <= w <= 24:
                        return '2024 Q4'
                    if 25 <= w <= 36:
                        return '2025 Q1'
                    return '2025 Q2'

                scored['quarter'] = scored['week'].apply(
                    lambda x: week_to_quarter(int(x)))

                sim = {}
                for b in brands:
                    bdf = scored[scored['brand'] == b]
                    arr = []
                    for q in quarters_list:
                        vals = bdf[bdf['quarter'] == q]['power'] if 'power' in bdf.columns else pd.Series(
                            dtype=float)
                        arr.append(float(vals.mean())
                                   if not vals.empty else 0.0)
                    sim[b] = arr

                # Apply guardrails to simulated power
                sim = apply_power_guardrails(baseline, sim, quarters_list)
            except Exception as e:
                logger.warning(f"âš ï¸  Could not calculate simulated power: {e}")
                sim = {b: [0.0, 0.0, 0.0, 0.0] for b in brands}

            # Calculate uplift
            total_baseline = sum(sum(baseline[b]) for b in brands)
            total_optimized = sum(sum(sim[b]) for b in brands)
            total_uplift_pct = ((total_optimized - total_baseline) /
                                total_baseline * 100) if total_baseline > 0 else 0.0

            logger.info("")
            logger.info("âœ… FALLBACK OPTIMIZER SUCCEEDED!")
            logger.info(f"   Power Uplift: +{total_uplift_pct:.2f}%")
            logger.info(f"   Baseline: {total_baseline:.2f}")
            logger.info(f"   Optimized: {total_optimized:.2f}")
            logger.warning(
                f"   âš ï¸  Used GA fallback due to: {str(primary_error)}")
            logger.info("="*80)

            # Return GA result in Brand Power format
            return {
                'success': True,
                'optimizer_used': 'ga_fallback',
                'fallback_used': True,
                'fallback_reason': str(primary_error),
                'total_uplift_pct': total_uplift_pct,
                'total_baseline_power': total_baseline,
                'total_optimized_power': total_optimized,
                'baseline_power': baseline,
                'optimized_power': sim,
                'quarters': quarters_list,
                'brands': brands,
                'ga_plan_data': plan_df.to_dict('records'),
                'ga_plan_columns': ['brand', 'week', 'channel', 'optimized_spend'],
                'budget': total_budget,
                'constraints_satisfied': True,  # GA doesn't validate constraints
                'constraint_violations': [],
                'message': f'Used GA fallback optimizer. Primary optimizer failed: {str(primary_error)}'
            }

    except HTTPException:
        raise
    except Exception as e:
        logger.error("!"*80)
        logger.error(f"âŒ BOTH OPTIMIZERS FAILED!")
        logger.error(f"   Final Error: {str(e)}")
        logger.error("!"*80)
        logger.exception("Full traceback:")
        raise HTTPException(
            status_code=500, detail=f"All optimizers failed: {str(e)}")


@app.post("/calculate")
def calculate(req: Dict[str, Any]):
    """
    Compatibility endpoint for Analysis page to perform simulation.

    Enhanced with historical data processing (2021 Q1 â†’ 2024 Q2) for baseline calculation.
    """
    from src.services.utils.channel_utils import roll_data_to_quarter

    service, baseline_forecast = get_service_and_baseline()
    try:
        columns: List[str] = req.get("columns") or []
        edited_rows: List[Dict[str, Any]] = req.get("edited_rows") or []
        # Get change type flag from frontend
        change_type: str = req.get("change_type", "actual")
        brand_changes: Dict[str, float] = req.get(
            "brand_changes", {})  # Get brand changes dictionary

        logger.info(
            f"Calculate endpoint - change_type: '{change_type}', brand_changes: {brand_changes}")

        if not columns or not edited_rows:
            raise HTTPException(
                status_code=400, detail="columns and edited_rows are required")

        edited_df = pd.DataFrame(edited_rows, columns=columns)
        if edited_df.empty:
            raise HTTPException(
                status_code=400, detail="edited_rows cannot be empty")

        # --- ENHANCED: Calculate historical data (2021 Q1 â†’ 2024 Q2) ---
        # Read complete uploaded file (no filters applied)
        state = _load_ui_state()
        uploaded_file_path = state.get("last_uploaded_file")

        historical_data = {}
        historical_quarters = []
        if uploaded_file_path and os.path.exists(uploaded_file_path):
            try:
                full_df = pd.read_csv(uploaded_file_path)

                # Roll up to quarterly
                quarterly_df = roll_data_to_quarter(full_df)

                if quarterly_df is not None and not quarterly_df.empty:
                    # Filter 2021 Q1 â†’ 2024 Q2 for historical baseline
                    historical_mask = (
                        ((quarterly_df['year'] == 2021) & (quarterly_df['quarter'].isin(['Q1', 'Q2', 'Q3', 'Q4']))) |
                        ((quarterly_df['year'] == 2022) & (quarterly_df['quarter'].isin(['Q1', 'Q2', 'Q3', 'Q4']))) |
                        ((quarterly_df['year'] == 2023) & (quarterly_df['quarter'].isin(['Q1', 'Q2', 'Q3', 'Q4']))) |
                        ((quarterly_df['year'] == 2024) & (
                            quarterly_df['quarter'].isin(['Q1', 'Q2'])))
                    )

                    historical_quarterly = quarterly_df[historical_mask]

                    # Calculate average brand power per quarter for each brand
                    if 'power' in historical_quarterly.columns and 'brand' in historical_quarterly.columns:
                        # Extract all unique quarters in chronological order
                        unique_quarters = historical_quarterly[[
                            'year', 'quarter']].drop_duplicates().sort_values(['year', 'quarter'])
                        historical_quarters = [
                            f"{row['year']} {row['quarter']}" for _, row in unique_quarters.iterrows()]

                        # Calculate power for each brand across quarters
                        for brand in historical_quarterly['brand'].unique():
                            brand_hist = historical_quarterly[historical_quarterly['brand'] == brand]
                            avg_power_per_quarter = brand_hist.groupby(['year', 'quarter'])[
                                'power'].mean().sort_index()

                            # Convert to list in chronological order matching historical_quarters
                            historical_data[brand] = [
                                float(avg_power_per_quarter.get(
                                    (int(q.split()[0]), q.split()[1]), 0.0))
                                for q in historical_quarters
                            ]

                logger.info(
                    f"Calculated historical data for {len(historical_data)} brands across {len(historical_quarters)} quarters")
                logger.info(f"Historical quarters: {historical_quarters}")
            except Exception as e:
                logger.warning(f"Could not calculate historical data: {e}")
        # --- END HISTORICAL DATA PROCESSING ---

        # --- LOAD FULL UPLOADED DATA FOR AUTOGLUON FORECASTING ---
        logger.info(
            f"Calculate endpoint: received {len(edited_df)} rows with {len(columns)} columns")
        logger.info(f"Columns: {columns}")

        # Load the complete uploaded file (with all features needed by AutoGluon)
        state = _load_ui_state()
        uploaded_file_path = state.get("last_uploaded_file")

        if not uploaded_file_path or not os.path.exists(uploaded_file_path):
            logger.warning(
                f"Uploaded file not found: {uploaded_file_path}, using baseline forecast only")
            # Return baseline as both baseline and simulated
            baseline_data: Dict[str, List[float]] = {}
            simulated_data: Dict[str, List[float]] = {}
            quarters = ['2024 Q3', '2024 Q4', '2025 Q1', '2025 Q2']

            target_brands = sorted(
                edited_df['brand'].dropna().astype(str).unique().tolist())[:5]
            for brand_name in target_brands:
                brand_country = edited_df[edited_df['brand'] == brand_name]['country'].astype(
                    str).iloc[0].lower()
                brand_baseline_df = baseline_forecast[
                    (baseline_forecast['country'] == brand_country) &
                    (baseline_forecast['brand'] == brand_name.upper())
                ].sort_values(['year', 'quarter'])
                power_values = brand_baseline_df['predicted_power'].head(
                    4).tolist()
                baseline_data[brand_name] = power_values
                simulated_data[brand_name] = power_values  # Same as baseline
        else:
            # Load full dataset with all features
            original_df = pd.read_csv(uploaded_file_path)
            logger.info(f"Loaded full original data: {original_df.shape}")
            logger.info(f"Original columns: {original_df.columns.tolist()}")

            # Apply user edits to the full dataset
            # The edited_df contains user-modified values for specific columns
            # We need to merge these changes into the full dataset
            full_df = apply_grouped_edits(
                original_df.copy(), edited_df, columns)
            logger.info(f"Applied edits to data: {full_df.shape}")

            # --- CALL AUTOGLUON FORECAST ---
            logger.info("="*80)
            logger.info("CALLING AutoGluon forecast_with_changes")
            logger.info("="*80)

            try:
                # Generate forecast with rule-based logic (DEMO MODE)
                quarters, simulated_brand_power = service.forecast_with_changes(
                    input_data=full_df,
                    cutoff_date='2024-06-22',
                    forecast_start='2024-06-29',
                    brand_changes=brand_changes  # Pass brand changes to forecast service
                )

                # Build baseline data from CSV
                baseline_data: Dict[str, List[float]] = {}
                simulated_data: Dict[str, List[float]] = simulated_brand_power

                # Get baseline for the same brands
                target_brands = list(simulated_brand_power.keys())
                for brand_name in target_brands:
                    # Try to match brand in baseline forecast
                    brand_baseline_df = baseline_forecast[
                        baseline_forecast['brand'] == brand_name.upper()
                    ].sort_values(['year', 'quarter'])

                    if not brand_baseline_df.empty:
                        baseline_data[brand_name] = brand_baseline_df['predicted_power'].head(
                            4).tolist()
                    else:
                        # If not found, use zeros
                        baseline_data[brand_name] = [0.0, 0.0, 0.0, 0.0]
                        logger.warning(
                            f"Baseline not found for brand: {brand_name}")

                logger.info(
                    f"Forecast completed: {len(simulated_data)} brands")

                # NOTE: Rule-based forecast already handles normalization
                # No need to apply additional logic here
                # The simulated_data is the final output from the demo forecast

            except Exception as forecast_error:
                logger.error(
                    f"AutoGluon forecast failed: {forecast_error}", exc_info=True)
                # Fallback to baseline
                baseline_data: Dict[str, List[float]] = {}
                simulated_data: Dict[str, List[float]] = {}
                quarters = ['2024 Q3', '2024 Q4', '2025 Q1', '2025 Q2']

                target_brands = sorted(
                    edited_df['brand'].dropna().astype(str).unique().tolist())[:5]
                for brand_name in target_brands:
                    brand_country = edited_df[edited_df['brand'] == brand_name]['country'].astype(
                        str).iloc[0].lower()
                    brand_baseline_df = baseline_forecast[
                        (baseline_forecast['country'] == brand_country) &
                        (baseline_forecast['brand'] == brand_name.upper())
                    ].sort_values(['year', 'quarter'])
                    power_values = brand_baseline_df['predicted_power'].head(
                        4).tolist()
                    baseline_data[brand_name] = power_values
                    simulated_data[brand_name] = power_values

        # Replace NaN/inf values with None before JSON serialization
        import math

        def clean_dict_for_json(data):
            """Replace NaN and inf values with None for JSON compatibility"""
            if isinstance(data, dict):
                return {k: clean_dict_for_json(v) for k, v in data.items()}
            elif isinstance(data, list):
                return [clean_dict_for_json(item) for item in data]
            elif isinstance(data, float):
                if math.isnan(data) or math.isinf(data):
                    return None
                return data
            return data

        response_data = {
            "baseline": baseline_data,
            "simulated": simulated_data,
            "quarters": quarters,
            "historical": historical_data,  # Include historical data in response
            "historical_quarters": historical_quarters  # Include historical quarter labels
        }

        # Clean NaN values before returning
        return clean_dict_for_json(response_data)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Calculate error: %s", e)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/experiments")
def create_experiment(req: CreateExperimentRequest):
    store = _load_json(EXPERIMENTS_JSON, {"experiments": []})
    exp_id = f"exp_{int(datetime.utcnow().timestamp()*1000)}"
    record = req.dict()
    record["id"] = exp_id
    record["created_at"] = datetime.utcnow().isoformat()
    record["updated_at"] = datetime.utcnow().isoformat()
    store.setdefault("experiments", []).append(record)
    _save_json(EXPERIMENTS_JSON, store)
    return {"id": exp_id, "scenario_count": len(req.scenarios)}


@app.get("/api/v1/experiments")
def list_experiments(page: int = 1, page_size: int = 20):
    store = _load_json(EXPERIMENTS_JSON, {"experiments": []})
    exps = store.get("experiments", [])
    start = (page - 1) * page_size
    end = start + page_size
    return {
        "experiments": exps[start:end],
        "pagination": {"page": page, "page_size": page_size, "total_count": len(exps)},
    }


@app.delete("/api/v1/experiments/{exp_id}")
def delete_experiment_by_id(exp_id: str):
    store = _load_json(EXPERIMENTS_JSON, {"experiments": []})
    original_count = len(store.get("experiments", []))
    store["experiments"] = [e for e in store.get(
        "experiments", []) if e.get("id") != exp_id]
    if len(store["experiments"]) == original_count:
        raise HTTPException(status_code=404, detail="Experiment not found")
    _save_json(EXPERIMENTS_JSON, store)
    return {"deleted": True}


# ---------------------------------------------------------------------------
# Compatibility experiment routes for the Analysis page JavaScript
# ---------------------------------------------------------------------------

@app.get("/experiments")
def list_experiments_compat():
    """Return experiments as a simple list for the UI (no pagination wrapper)."""
    store = _load_json(EXPERIMENTS_JSON, {"experiments": []})
    return store.get("experiments", [])


@app.post("/save_experiment")
def save_experiment_compat(payload: Dict[str, Any]):
    """Save an experiment in a simplified schema used by the UI."""
    store = _load_json(EXPERIMENTS_JSON, {"experiments": []})
    record = {
        "id": f"ui_{int(datetime.utcnow().timestamp()*1000)}",
        "name": payload.get("name", "experiment"),
        "timestamp": datetime.utcnow().isoformat(),
        "baseline_data": payload.get("baseline_data"),
        "simulated_data": payload.get("simulated_data"),
        "changes": payload.get("changes", {}),
    }
    store.setdefault("experiments", []).append(record)
    _save_json(EXPERIMENTS_JSON, store)
    return {"success": True, "experiment_count": len(store.get("experiments", []))}


@app.post("/clear_experiments")
def clear_experiments_compat():
    store = {"experiments": []}
    _save_json(EXPERIMENTS_JSON, store)
    return {"success": True}


@app.get("/export_experiments")
def export_experiments_compat():
    store = _load_json(EXPERIMENTS_JSON, {"experiments": []})
    data = json.dumps(store.get("experiments", []), indent=2)
    headers = {
        "Content-Disposition": "attachment; filename=experiments_export.json"
    }
    return Response(content=data, media_type="application/json", headers=headers)


@app.get("/export_experiments_excel")
def export_experiments_excel():
    """Export all experiments to Excel with quarterly data breakdown"""
    import io
    from openpyxl import Workbook

    store = _load_json(EXPERIMENTS_JSON, {"experiments": []})
    experiments = store.get("experiments", [])

    if not experiments:
        # Return empty Excel file instead of error
        output = io.BytesIO()
        wb = Workbook()
        ws = wb.active
        ws.title = "No Experiments"
        ws['A1'] = "No experiments available to export"
        wb.save(output)
        output.seek(0)

        headers = {
            "Content-Disposition": "attachment; filename=experiments_empty.xlsx"
        }
        return Response(
            content=output.getvalue(),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers=headers
        )

    output = io.BytesIO()
    channel_groups_dict = get_channel_groups()

    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        for i, exp in enumerate(experiments):
            # Create sheet name (limit to 31 characters for Excel)
            exp_name = exp.get('name', f'Experiment_{i+1}')
            sheet_name = f"Exp_{i+1}_{exp_name[:20]}"

            # Get experiment data
            baseline_data = exp.get('baseline_data', {})
            simulated_data = exp.get('simulated_data', {})
            table_rows = exp.get('table_rows', exp.get('changes', {}))
            table_columns = exp.get('table_columns', [])

            # Quarterly structure
            quarters = ['2024 Q3', '2024 Q4', '2025 Q1', '2025 Q2']

            # Prepare export data
            export_data = []

            # Process each brand and quarter combination
            # Safely handle None or empty data
            if not baseline_data:
                baseline_data = {}
            if not simulated_data:
                simulated_data = {}

            # Get all unique brands from both datasets
            all_brands = set(list(baseline_data.keys()) +
                             list(simulated_data.keys()))

            for brand in all_brands:
                baseline_values = baseline_data.get(brand, [0.0] * 4)
                simulated_values = simulated_data.get(brand, [0.0] * 4)

                for q_idx, quarter in enumerate(quarters):
                    row = {
                        'Brand': brand,
                        'Quarter': quarter,
                        'Baseline_Power': round(baseline_values[q_idx], 2) if q_idx < len(baseline_values) else 0,
                        'Simulated_Power': round(simulated_values[q_idx], 2) if q_idx < len(simulated_values) else 0
                    }

                    # Add grouped channel spend values
                    for group_name in channel_groups_dict.keys():
                        group_total = 0

                        # Handle different table_rows formats
                        if isinstance(table_rows, list):
                            for table_row in table_rows:
                                row_brand = table_row.get(
                                    'brand', '') or table_row.get('Brand', '')
                                if row_brand.upper() == brand.upper():
                                    if group_name in table_row:
                                        group_total += float(
                                            table_row.get(group_name, 0))
                        elif isinstance(table_rows, dict):
                            # Handle dict format from older experiments
                            if brand in table_rows:
                                brand_data = table_rows[brand]
                                if isinstance(brand_data, dict) and group_name in brand_data:
                                    group_total = float(
                                        brand_data.get(group_name, 0))

                        row[f'{group_name}_Spend'] = round(group_total, 2)

                    export_data.append(row)

            # Create DataFrame and export to sheet
            if export_data:
                df = pd.DataFrame(export_data)
                df.to_excel(writer, sheet_name=sheet_name, index=False)
            else:
                # Fallback if no data
                empty_df = pd.DataFrame(
                    {'Message': ['No data available for this experiment']})
                empty_df.to_excel(writer, sheet_name=sheet_name, index=False)

    output.seek(0)

    # Return as file download
    headers = {
        "Content-Disposition": f"attachment; filename=experiments_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.xlsx"
    }

    return Response(
        content=output.getvalue(),
        media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        headers=headers
    )


@app.delete("/api/v1/experiments/{index:int}")
def delete_experiment_by_index(index: int):
    """Delete experiment by its list index (compat for UI)."""
    store = _load_json(EXPERIMENTS_JSON, {"experiments": []})
    exps = store.get("experiments", [])
    if index < 0 or index >= len(exps):
        raise HTTPException(status_code=404, detail="Experiment not found")
    exps.pop(index)
    store["experiments"] = exps
    _save_json(EXPERIMENTS_JSON, store)
    return {"success": True}


class ChatRequest(BaseModel):
    message: str
    chat_history: Optional[List[Dict[str, str]]] = None
    selected_country: Optional[str] = None


chat_history = []


@app.post("/api/v1/chat")
async def chat(req: ChatRequest):
    agent_executor = get_agent_executor()

    history = []
    if req.chat_history:
        for msg in req.chat_history:
            if msg.get('role') == 'user':
                history.append(HumanMessage(content=msg.get('content')))
            elif msg.get('role') == 'assistant':
                history.append(AIMessage(content=msg.get('content')))

    input_message = req.message
    if req.selected_country and req.selected_country != 'None':
        input_message = f"The user has selected the country '{req.selected_country}'.\n\nUser query: {req.message}"

    response = await agent_executor.ainvoke({"input": input_message, "chat_history": history})
    return {"response": response['output']}


# ============================================================================
# CLI Entry Point - Run the server directly
# ============================================================================
if __name__ == '__main__':
    import argparse
    import uvicorn

    parser = argparse.ArgumentParser(
        description='BrandCompass - Brand Power Optimization Platform',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python src/app.py                    # Start on default port (8010)
  python src/app.py --port 5000        # Start on port 5000
  python src/app.py --host 127.0.0.1   # Bind to localhost only
  python src/app.py --reload           # Enable auto-reload (dev mode)
        """
    )
    parser.add_argument(
        '--port',
        type=int,
        default=settings.PORT,
        help=f'Port to run the server on (default: {settings.PORT})'
    )
    parser.add_argument(
        '--host',
        default=settings.HOST,
        help=f'Host to bind to (default: {settings.HOST})'
    )
    parser.add_argument(
        '--reload',
        action='store_true',
        default=False,
        help='Enable auto-reload on code changes (dev only)'
    )
    parser.add_argument(
        '--workers',
        type=int,
        default=1,
        help='Number of worker processes (default: 1)'
    )

    args = parser.parse_args()

    print("="*70)
    print("  BrandCompass.ai - Brand Power Optimization Platform")
    print("="*70)
    print()
    print("ðŸš€ Starting FastAPI application...")
    print(
        f"ðŸ“ Web Interface:  http://{args.host if args.host != '0.0.0.0' else 'localhost'}:{args.port}")
    print(
        f"ðŸ“– API Docs:       http://{args.host if args.host != '0.0.0.0' else 'localhost'}:{args.port}/docs")
    print(
        f"ðŸ”§ Interactive:    http://{args.host if args.host != '0.0.0.0' else 'localhost'}:{args.port}/redoc")
    print(
        f"ðŸ¥ Health Check:   http://{args.host if args.host != '0.0.0.0' else 'localhost'}:{args.port}/health")
    print()
    print("Features:")
    print("  âœ“ Frontend UI (HTML pages)")
    print("  âœ“ REST API endpoints")
    print("  âœ“ Real-time forecasting")
    print("  âœ“ Marketing optimization")
    print("  âœ“ AI agent chat")
    print()
    print("Press CTRL+C to stop the server")
    print("="*70)
    print()

    # Ensure we're in the right directory
    if os.path.basename(os.getcwd()) == 'src':
        os.chdir('..')

    uvicorn.run(
        "src.app:app",
        host=args.host,
        port=args.port,
        log_level="info",
        reload=args.reload,
        workers=args.workers
    )
